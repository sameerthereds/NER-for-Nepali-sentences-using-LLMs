{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mistralai\n",
    "# !pip install huggingface_hub\n",
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install openai\n",
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "# Set up OpenAI API key\n",
    "client_openai=  OpenAI(api_key=\"\",\n",
    ")\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "client_mistral = Mistral(api_key=\"\")\n",
    "\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# system_prompt_nepali = \"तपाईं एक उत्कृष्ट भाषाविज्ञ हुनुहुन्छ। \"\n",
    "\n",
    "def generate_NER(model_name,prompt):\n",
    "\n",
    "\n",
    "    generated_text=\"\"\n",
    "\n",
    "    if model_name==\"openai\":\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model = 'gpt-4o',\n",
    "            messages = [\n",
    "                {'role':'system',\"content\": \"You are an excellent linguist. \"},\n",
    "            #   {'role':'system',\"content\": \"तपाईं एक उत्कृष्ट भाषाविज्ञ हुनुहुन्छ। \"},\n",
    "                {'role': 'user', 'content':prompt}\n",
    "            ],\n",
    "            # temperature = 0  ,\n",
    "                max_tokens=500,\n",
    "                temperature=0,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  n=1\n",
    "            )\n",
    "\n",
    "        generated_text = completion.choices[0].message.content.strip()\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nowalab/everest-ner.git\n",
    "!git clone https://github.com/nowalab/DanfeNER.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "  with open(file_path,\"r\") as f:\n",
    "    tokens,labels = [],[]\n",
    "    t,l = [], []\n",
    "    for line in f.readlines():\n",
    "      tmp = line.strip().split()\n",
    "      if len(tmp) == 0:\n",
    "        tokens.append(t)\n",
    "        labels.append(l)\n",
    "        t, l = [], []\n",
    "      else:\n",
    "        t.append(tmp[0])\n",
    "        l.append(tmp[1])\n",
    "    if len(t) > 0:\n",
    "      tokens.append(t)\n",
    "      labels.append(l)\n",
    "    data = tokens,labels\n",
    "    return data\n",
    "\n",
    "def get_news_data_sets():\n",
    "  train_data= parse_file(\"everest-ner/EverestNER-train-bio.txt\")\n",
    "  test_data= parse_file(\"everest-ner/EverestNER-test-bio.txt\")\n",
    "  return train_data,test_data\n",
    "\n",
    "def get_tweets_data_sets():\n",
    "  train_data = parse_file(\"DanfeNER/DanfeNER-train-bio.txt\")\n",
    "  test_data = parse_file(\"DanfeNER/DanfeNER-test-bio.txt\")\n",
    "  return train_data,test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list=[\"Location\",\"Date\",\"Person\",\"Organization\",\"Event\"]\n",
    "\n",
    "english_entities_dict={\"Location\":\"Location\",\n",
    "                       \"Date\":\"Date\",\n",
    "                       \"Person\":\"Person\",\n",
    "                       \"Organization\":\"Organization\",\n",
    "                       \"Event\":\"Event\"}\n",
    "\n",
    "\n",
    "nepali_entities_dict={\"Location\":\"स्थानको नाम\",\n",
    "                       \"Date\":\"मिति या समय\",\n",
    "                       \"Person\":\"व्यक्तिको नाम\",\n",
    "                       \"Organization\":\"सङ्घ संस्थाको नाम\",\n",
    "                       \"Event\":\"उत्सव, पर्व या घटनाको नाम\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train,news_test=get_news_data_sets()\n",
    "tweet_train,tweet_test=get_tweets_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train_sentences, news_train_labels = news_train\n",
    "news_test_sentences, news_test_labels = news_test\n",
    "len(news_train_sentences),len(news_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train_sentences, tweet_train_labels = tweet_train\n",
    "tweet_test_sentences, tweet_test_labels = tweet_test\n",
    "len(tweet_train_sentences),len(tweet_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_train_sentences[0],tweet_train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_examples_with_correct_entity_spans(examples):\n",
    "    \"\"\"\n",
    "    Process a list of examples and return dictionaries where keys are entity types,\n",
    "    and values are the whole sentence with only the specific entity type annotated.\n",
    "\n",
    "    Args:\n",
    "        examples (list): A list of examples, where each example is a tuple of tokens and labels.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, one for each sentence, where keys are entity types\n",
    "              and values are the whole sentence with only the specific entity type annotated.\n",
    "    \"\"\"\n",
    "    all_outputs = []\n",
    "\n",
    "    for tokens, labels in examples:\n",
    "        # Initialize the output dictionary with the original sentence for each tag\n",
    "        output = {\n",
    "            \"Location\": \" \".join(tokens),\n",
    "            \"Date\": \" \".join(tokens),\n",
    "            \"Person\": \" \".join(tokens),\n",
    "            \"Organization\": \" \".join(tokens),\n",
    "            \"Event\": \" \".join(tokens)\n",
    "        }\n",
    "\n",
    "        # Process the sentence for each tag individually\n",
    "        for entity_type in output.keys():\n",
    "            annotated_tokens = tokens[:]  # Copy of the tokens for this tag\n",
    "            in_entity = False\n",
    "            start_index = None\n",
    "\n",
    "            for i, t in enumerate(tokens):\n",
    "                tag = labels[i]\n",
    "\n",
    "                # Start of the specific entity type\n",
    "                if tag == f\"B-{entity_type}\":\n",
    "                    if in_entity:  # If already in an entity, close the previous one\n",
    "                        annotated_tokens[start_index] = f\"@@{annotated_tokens[start_index]}\"\n",
    "                        annotated_tokens[i - 1] += \"##\"\n",
    "                    start_index = i\n",
    "                    in_entity = True\n",
    "\n",
    "                # End of the specific entity or outside any entity\n",
    "                elif in_entity and (tag != f\"I-{entity_type}\" or i == len(tokens) - 1):\n",
    "                    # Close the current entity\n",
    "                    annotated_tokens[start_index] = f\"@@{annotated_tokens[start_index]}\"\n",
    "                    if tag != f\"I-{entity_type}\":  # If not continuing, close the entity\n",
    "                        annotated_tokens[i - 1] += \"##\"\n",
    "                        in_entity = False\n",
    "                    elif i == len(tokens) - 1:  # Handle the last token\n",
    "                        annotated_tokens[i] += \"##\"\n",
    "                        in_entity = False\n",
    "\n",
    "            # Close any lingering entity\n",
    "            if in_entity:\n",
    "                annotated_tokens[start_index] = f\"@@{annotated_tokens[start_index]}\"\n",
    "                annotated_tokens[len(tokens) - 1] += \"##\"\n",
    "\n",
    "            # Combine tokens to form the annotated sentence\n",
    "            output[entity_type] = \" \".join(annotated_tokens)\n",
    "\n",
    "        all_outputs.append(output)\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_datasets_with_tagging={}\n",
    "for i in range(0,len(news_train_sentences)):\n",
    "  tokens, labels = news_train_sentences[i],news_train_labels[i]\n",
    "#   print(tokens,labels)\n",
    "  annotated_sentences = process_examples_with_correct_entity_spans([(tokens,labels)])\n",
    "  annotated_sentences.append(labels)\n",
    "  train_datasets_with_tagging[\" \".join(tokens)]=annotated_sentences\n",
    "\n",
    "\n",
    "test_datasets_with_tagging={}\n",
    "for i in range(0,len(news_test_sentences)):\n",
    "  tokens, labels = news_test_sentences[i],news_test_labels[i]\n",
    "#   print(tokens,labels)\n",
    "  annotated_sentences = process_examples_with_correct_entity_spans([(tokens,labels)])\n",
    "  annotated_sentences.append(labels)\n",
    "  test_datasets_with_tagging[\" \".join(tokens)]=annotated_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "len(train_datasets_with_tagging),len(test_datasets_with_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/output_path/train_datasets_with_tagging.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_datasets_with_tagging, file)\n",
    "\n",
    "with open(\"/output_path/test_datasets_with_tagging.pkl\", \"wb\") as file:\n",
    "    pickle.dump(test_datasets_with_tagging, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_path/train_datasets_with_tagging.pkl\", \"rb\") as file:  # \"rb\" stands for read binary\n",
    "    train_datasets_with_tagging = pickle.load(file)\n",
    "\n",
    "with open(\"output_path/test_datasets_with_tagging.pkl\", \"rb\") as file:  # \"rb\" stands for read binary\n",
    "    test_datasets_with_tagging = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_random_k_examples_with_entity(k,entity):\n",
    "    entity_data={}\n",
    "    \n",
    "    for i in train_datasets_with_tagging:\n",
    "        if \"@\" in train_datasets_with_tagging[i][0][entity]:\n",
    "            entity_data[i]=[train_datasets_with_tagging[i][0][entity],train_datasets_with_tagging[i][1]]\n",
    "            \n",
    "            \n",
    "    selected_keys = random.sample(list(entity_data.keys()), k)    \n",
    "    k_data = {key: entity_data[key] for key in selected_keys}\n",
    "\n",
    "    return k_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_random_k_examples={}\n",
    "for entity in entities_list:\n",
    "    entity_k={}\n",
    "    \n",
    "    temp_examples=get_random_k_examples_with_entity(100,entity)\n",
    "    # entity_k[100]=temp_examples\n",
    "    entity_random_k_examples[entity]=temp_examples\n",
    "\n",
    "with open(\"output_path/entity_random_k_examples.pkl\", \"wb\") as file:\n",
    "    pickle.dump(entity_random_k_examples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_random_k_examples_without_entity(k):\n",
    "    entity_data={}\n",
    "    \n",
    "    for i in train_datasets_with_tagging:\n",
    "        for entity in train_datasets_with_tagging[i][0]:\n",
    "            if \"@\" in train_datasets_with_tagging[i][0][entity]:\n",
    "                entity_data[i]=[train_datasets_with_tagging[i][0][entity],train_datasets_with_tagging[i][1]]\n",
    "            \n",
    "            \n",
    "    selected_keys = random.sample(list(entity_data.keys()), k)    \n",
    "    k_data = {key: entity_data[key] for key in selected_keys}\n",
    "\n",
    "    return k_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"path/bert-npvec1/tokenizer\")\n",
    "model = BertModel.from_pretrained(\"path/bert-npvec1\", output_hidden_states=True)\n",
    "\n",
    "# Function to get sentence embeddings\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token as sentence embedding\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding.squeeze().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [ \" \".join(i) for i in news_train_sentences]\n",
    "train_embeddings = np.array([get_sentence_embedding(sentence) for sentence in sentences])\n",
    "\n",
    "\n",
    "with open(\"output_path/train_embeddings.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_embeddings, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_path/train_embeddings.pkl\", \"rb\") as file:  # \"rb\" stands for read binary\n",
    "    train_embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get top k most similar sentences\n",
    "def get_top_k_similar_sentences(query, embeddings,sentences, k,train_datasets_with_tagging,entity):\n",
    "    # Compute embeddings for all sentences\n",
    "   \n",
    "    \n",
    "    # Compute embedding for the query sentence\n",
    "    query_embedding = get_sentence_embedding(query)\n",
    "    \n",
    "    # Compute cosine similarity between query and all sentences\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "    \n",
    "    # Get indices of top k similar sentences\n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]  # Sort in descending order of similarity\n",
    "    \n",
    "    # Return top k sentences as a list\n",
    "    top_k_sentences = [sentences[idx] for idx in top_k_indices]\n",
    "    top_k_annotated_sentences = [train_datasets_with_tagging[i][0] for i in top_k_sentences]\n",
    "    top_k_labels = [train_datasets_with_tagging[i][1] for i in top_k_sentences]\n",
    "\n",
    "    final_dict={}\n",
    "    for i in range(len(top_k_sentences)):\n",
    "        final_dict[top_k_sentences[i]]=[top_k_annotated_sentences[i][entity],top_k_labels[i]]\n",
    "    return final_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_sentences={}\n",
    "for entity in entities_list:\n",
    "    temp_examples=[]\n",
    "    for i in train_datasets_with_tagging:\n",
    "        if \"@\" in train_datasets_with_tagging[i][0][entity]:\n",
    "            temp_examples.append(i)\n",
    "    entity_sentences[entity]=temp_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [ \" \".join(i) for i in news_train_sentences]\n",
    "entity_NN_k_examples_testsentences={}\n",
    "\n",
    "for entity in entities_list:     \n",
    "    temp_examples=entity_sentences[entity]            \n",
    "    sentence_indexes=[sentences.index(i) for i in temp_examples]            \n",
    "    selected_sentences=[sentences[i] for i in sentence_indexes]\n",
    "    selected_embeddings=[train_embeddings[i] for i in sentence_indexes]\n",
    "    entity_wise={}\n",
    "    count=0\n",
    "    for test_sentence in test_datasets_with_tagging:   \n",
    "        similar_sentences=get_top_k_similar_sentences(test_sentence, selected_embeddings,selected_sentences,100,train_datasets_with_tagging,entity)\n",
    "        entity_wise[test_sentence]=similar_sentences    \n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print(entity,count)\n",
    "    entity_NN_k_examples_testsentences[entity]=entity_wise\n",
    "\n",
    "with open(\"output_path/entity_NN_k_examples_testsentences.pkl\", \"wb\") as file:\n",
    "    pickle.dump(entity_NN_k_examples_testsentences, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_path/entity_random_k_examples.pkl\", \"rb\") as file:  # \"rb\" stands for read binary\n",
    "    entity_random_k_examples = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/sneupane/NER/Flairs_paper/train_examples/entity_NN_k_examples_testsentences.pkl\", \"rb\") as file:  # \"rb\" stands for read binary\n",
    "    entity_NN_k_examples_testsentences = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_with_k_NN_examples(k,entity,entity_prompt,test_sentence):\n",
    "\n",
    "    # person_prompt =\"The task is to label \" + entity_prompt + \" entities in the given Nepali sentence.\"\n",
    "    person_prompt =\"गर्नुपर्ने काम भनेको दिइएको नेपाली वाक्यमा \" + entity_prompt + \"लाई @@ ## भित्र लेबल गर्नु हो।\"\n",
    "    if k > 0:\n",
    "        #  person_prompt+=\" Below are some examples with Input and Output pairs. For the prediction, you should generate the output in the same format as in the examples. Do not give any explanations. \\n Examples:\"\n",
    "        person_prompt+=\"तल वाक्य र लेबल गरेका नतिजाका केही उदाहरणहरू दिइएका छन्। वाक्यलाई लेबल गर्दा उदाहरणको जस्तै ढाँचामा मात्र गर्नुहोस्। कुनै थप व्याख्या नगर्नुहोस्। \\n उदाहरणहरू:  \"\n",
    "    else:\n",
    "        person_prompt+=\" Output the whole sentence and enclose the entity within @@ and ##.\"\n",
    "        #  person_prompt+=\" पुरा वाक्यनै नतिजामा राख्नुहोस् र लेबललाई @@ र ## भित्र राख्नुहोस्।\"\n",
    "\n",
    "    k_nn_examples=[]\n",
    "    count=0\n",
    "    for i in entity_NN_k_examples_testsentences[entity][test_sentence]:\n",
    "        k_nn_examples.append([i,entity_NN_k_examples_testsentences[entity][test_sentence][i][0]])\n",
    "        count+=1\n",
    "        if count ==k :\n",
    "            break\n",
    " \n",
    "    if k==0:\n",
    "        person_prompt+=\"\\n\" + \"Input: \" + test_sentence + \"\\n\" + \"Output: \"\n",
    "        # person_prompt += \"\\n\" + \"वाक्य: \" + test_sentence + \"\\n\" + \"नतिजा: \"\n",
    "    else:\n",
    "        for i in k_nn_examples:\n",
    "            \n",
    "            # person_prompt+=\"\\n\" + \"Input: \" + i[0] + \"\\n\" + \"Output: \"+ i[1] + \"\\n\"\n",
    "            person_prompt += \"\\n\" + \"वाक्य: \" + i[0] + \"\\n\" + \"नतिजा: \"+ i[1] + \"\\n\"\n",
    "        # person_prompt+=\"\\n\" + \" Now predict the output for the following input sentence. \\n Input: \" + test_sentence + \"\\n\" \n",
    "        person_prompt += \"\\n\" + \"अब तल दिईएको वाक्यलाई लेबल गर्नुहोस्।  \\nवाक्य: \" + test_sentence + \"\\n\" \n",
    "    return person_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_prompt_with_k_random_examples(k,entity,entity_prompt,test_sentence):\n",
    "\n",
    "    # person_prompt =\"The task is to label \" + entity_prompt + \" entities in the given Nepali sentence.\"\n",
    "    person_prompt =\"गर्नुपर्ने काम भनेको दिइएको नेपाली वाक्यमा \" + entity_prompt + \"लाई @@ ## भित्र लेबल गर्नु हो।\"\n",
    "    if k > 0:\n",
    "        #  person_prompt+=\"  Below are some examples with Input and Output pairs. \\nFor the prediction, you should generate the output in the same format as in the examples.  Do not give any explanations. \\nExamples:\"\n",
    "        person_prompt+=\"तल वाक्य र लेबल गरेका नतिजाका केही उदाहरणहरू दिइएका छन्।\\nवाक्यलाई लेबल गर्दा उदाहरणको जस्तै ढाँचामा मात्र गर्नुहोस्। कुनै थप व्याख्या नगर्नुहोस्। \\nउदाहरणहरू: \"\n",
    "    else:\n",
    "        person_prompt+=\" Output the whole sentence and enclose the entity within @@ and ##.\"\n",
    "        # person_prompt+=\"पुरा वाक्यनै नतिजामा राख्नुहोस् र लेबललाई @@ र ## भित्र राख्नुहोस्।\"\n",
    "\n",
    "    random_k_examples=[]\n",
    "    count=0\n",
    "    for i in entity_random_k_examples[entity]:\n",
    "        random_k_examples.append([i,entity_random_k_examples[entity][i][0]])\n",
    "        count+=1\n",
    "        if count ==k :\n",
    "            break\n",
    "\n",
    "    if k==0:\n",
    "        person_prompt+=\"\\n\" + \"Input: \" + test_sentence + \"\\n\" + \"Output: \"\n",
    "        # person_prompt += \"\\n\" + \"वाक्य: \" + test_sentence + \"\\n\" + \"नतिजा: \"\n",
    "    else:\n",
    "        for i in random_k_examples:\n",
    "            \n",
    "            # person_prompt+=\"\\n\" + \"Input: \" + i[0] + \"\\n\" + \"Output: \"+ i[1] + \"\\n\"\n",
    "            person_prompt += \"\\n\" + \"वाक्य: \" + i[0] + \"\\n\" + \"नतिजा: \"+ i[1] + \"\\n\"\n",
    "        # person_prompt+= \"Now predict the output for the following input sentence. \\nInput: \" + test_sentence + \"\\n\" \n",
    "        person_prompt +=  \"अब तल दिईएको वाक्यलाई लेबल गर्नुहोस्। \\nवाक्य: \" + test_sentence + \"\\n\" \n",
    "    return person_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_prompt_with_k_random_examples(0,\"Person\",nepali_entities_dict[\"Person\"],\"उपाधि दौड मा रहे को इंग्लिस प्रिमियर लिग फुटबल क्लब लिभरपुल एफए कप को तेस्रो चरण बाटै बाहिरिए को छ ।\"))\n",
    "# print(generate_NER(\"openai\",a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
