{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Set up OpenAI API key\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "# Set up OpenAI API key\n",
    "client_openai=  OpenAI(api_key=\"\",\n",
    ")\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "client_mistral = Mistral(api_key=\"5O\")\n",
    "\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# system_prompt_nepali = \"तपाईं एक उत्कृष्ट भाषाविज्ञ हुनुहुन्छ। \"\n",
    "\n",
    "def generate_NER(model_name,prompt):\n",
    "\n",
    "\n",
    "    generated_text=\"\"\n",
    "\n",
    "    if model_name==\"openai\":\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model = 'gpt-4o',\n",
    "            messages = [\n",
    "                # {'role':'system',\"content\": \"You are an excellent linguist. \"},\n",
    "            #   {'role':'system',\"content\": \"तपाईं एक उत्कृष्ट भाषाविज्ञ हुनुहुन्छ। \"},\n",
    "                {'role': 'user', 'content':prompt}\n",
    "            ],\n",
    "            # temperature = 0  ,\n",
    "                max_tokens=500,\n",
    "                temperature=0,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  n=1\n",
    "            )\n",
    "\n",
    "        generated_text = completion.choices[0].message.content.strip()\n",
    "\n",
    "    \n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list=[\"Location\",\"Date\",\"Person\",\"Organization\",\"Event\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "  with open(file_path,\"r\") as f:\n",
    "    tokens,labels = [],[]\n",
    "    t,l = [], []\n",
    "    for line in f.readlines():\n",
    "      tmp = line.strip().split()\n",
    "      if len(tmp) == 0:\n",
    "        tokens.append(t)\n",
    "        labels.append(l)\n",
    "        t, l = [], []\n",
    "      else:\n",
    "        t.append(tmp[0])\n",
    "        l.append(tmp[1])\n",
    "    if len(t) > 0:\n",
    "      tokens.append(t)\n",
    "      labels.append(l)\n",
    "    data = tokens,labels\n",
    "    return data\n",
    "\n",
    "def get_news_data_sets():\n",
    "  train_data= parse_file(\"everest-ner/EverestNER-train-bio.txt\")\n",
    "  test_data= parse_file(\"everest-ner/EverestNER-test-bio.txt\")\n",
    "  return train_data,test_data\n",
    "\n",
    "def get_tweets_data_sets():\n",
    "  train_data = parse_file(\"DanfeNER/DanfeNER-train-bio.txt\")\n",
    "  test_data = parse_file(\"DanfeNER/DanfeNER-test-bio.txt\")\n",
    "  return train_data,test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train,news_test=get_news_data_sets()\n",
    "news_train_sentences, news_train_labels = news_train\n",
    "news_test_sentences, news_test_labels = news_test\n",
    "len(news_train_sentences),len(news_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio(entity, prediction,original_sentence):\n",
    "    tokens = prediction.split()\n",
    "    original_tokens=original_sentence.split()\n",
    "    bio_labels = []\n",
    "    inside_entity = False  # Tracks if we are inside an entity\n",
    "\n",
    "    for token in tokens:\n",
    "        if '@@' in token and '##' in token:  # Entire entity in one token\n",
    "            bio_labels.append(f\"B-{entity}\")\n",
    "            inside_entity = False\n",
    "        elif '@@' in token:  # Entity begins in this token\n",
    "            bio_labels.append(f\"B-{entity}\")\n",
    "            inside_entity = True\n",
    "        elif '##' in token:  # Entity ends in this token\n",
    "            bio_labels.append(f\"I-{entity}\")\n",
    "            inside_entity = False\n",
    "        else:\n",
    "            if inside_entity:  # Continuation of the entity\n",
    "                bio_labels.append(f\"I-{entity}\")\n",
    "            else:  # Outside of any entity\n",
    "                bio_labels.append(\"O\")\n",
    "    if len(bio_labels)==0:\n",
    "        bio_labels=[\"O\"]*len(original_tokens)\n",
    "\n",
    "    return bio_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_sentences(S, T):\n",
    "    \"\"\"\n",
    "    Aligns the predicted sentence T to match the reference sentence S\n",
    "    while preserving special tokens ('@@' and '##') in T.\n",
    "\n",
    "    Parameters:\n",
    "    S (str): The reference sentence.\n",
    "    T (str): The predicted sentence.\n",
    "\n",
    "    Returns:\n",
    "    str: The aligned version of T, matching the structure of S.\n",
    "    \"\"\"\n",
    "    # Split sentences into tokens\n",
    "    s_tokens = S.split()\n",
    "    t_tokens = T.split()\n",
    "\n",
    "    aligned_t_tokens = []\n",
    "    t_index = 0  # Pointer for T tokens\n",
    "\n",
    "    for s_token in s_tokens:\n",
    "        if t_index < len(t_tokens):\n",
    "            t_token = t_tokens[t_index]\n",
    "\n",
    "            # If the current token in T contains special markers, preserve it.\n",
    "            if '@@' in t_token or '##' in t_token:\n",
    "                aligned_t_tokens.append(t_token)\n",
    "                t_index += 1  # Move to the next token in T\n",
    "            else:\n",
    "                # Align tokens from T to match S\n",
    "                if t_token == s_token:\n",
    "                    aligned_t_tokens.append(t_token)\n",
    "                else:\n",
    "                    aligned_t_tokens.append(s_token)\n",
    "                t_index += 1\n",
    "        else:\n",
    "            # If T is shorter than S, pad with tokens from S\n",
    "            aligned_t_tokens.append(s_token)\n",
    "\n",
    "    # Handle any remaining tokens in T after exhausting S\n",
    "    while t_index < len(t_tokens):\n",
    "        aligned_t_tokens.append(t_tokens[t_index])\n",
    "        t_index += 1\n",
    "\n",
    "    # Join aligned tokens back into a sentence\n",
    "    return ' '.join(aligned_t_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_output(output):\n",
    "    prediction = output\n",
    "    \n",
    "    # Check if \"Output:\" is in the prediction and process accordingly\n",
    "    if \"Output:\" in prediction:\n",
    "        prediction = prediction.split(\"Output:\")[1].strip()\n",
    "\n",
    "    if \"नतिजा:\" in prediction:\n",
    "        prediction = prediction.split(\"नतिजा:\")[1].strip()\n",
    "\n",
    "    if \"वाक्य:\" in prediction:\n",
    "        prediction = prediction.split(\"वाक्य:\")[1].strip()\n",
    "    \n",
    "    # Extract portion before \"Note\" if it exists\n",
    "    if \"Note\" in prediction:\n",
    "        prediction = prediction.split(\"Note\", 1)[0].strip()\n",
    "    \n",
    "    # Extract up to the first occurrence of \"।\"\n",
    "    if \"।\" in prediction:\n",
    "        prediction = prediction.split(\"।\", 1)[0] + \"।\"\n",
    "    \n",
    "    # Return the processed prediction\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_path/train_datasets_with_tagging.pkl\", \"rb\") as file:  # \"rb\" stands for read binary\n",
    "    train_datasets_with_tagging = pickle.load(file)\n",
    "\n",
    "with open(\"output_path/test_datasets_with_tagging.pkl\", \"rb\") as file:  # \"rb\" stands for read binary\n",
    "    test_datasets_with_tagging = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_separated={}\n",
    "for sentence in test_datasets_with_tagging:\n",
    "    # if sentence == \"\\' माइती टाढा ।\":\n",
    "        gd=test_datasets_with_tagging[sentence][1]\n",
    "        a={}\n",
    "        for ent in entities_list:\n",
    "            temp=[\"O\"]*len(gd)\n",
    "            pos=[index for index, label in enumerate(gd) if ent in label]\n",
    "            if len(pos)>0:\n",
    "                for i in pos:\n",
    "                    temp[i]=gd[i]\n",
    "            # if len(temp)!=len(gd):\n",
    "                # print(\"I\")\n",
    "            a[ent]=temp\n",
    "        ground_truth_separated[sentence]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth_separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name=\"k_10_semantic_NEP.pkl\"\n",
    "\n",
    "with open(\"output_path/\"+output_file_name , \"rb\") as file:  # \"rb\" stands for read binary\n",
    "    k_10_semantic_NEP = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_random_keys(input_dict, num_keys=100, seed=42):\n",
    "    \"\"\"\n",
    "    Selects a specified number of keys randomly from a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        input_dict (dict): The dictionary to sample keys from.\n",
    "        num_keys (int): The number of keys to select.\n",
    "        seed (int): The seed for random number generator.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of randomly selected keys.\n",
    "    \"\"\"\n",
    "    random.seed(seed)  # Set the seed for reproducibility\n",
    "    all_keys = list(input_dict.keys())\n",
    "    \n",
    "    # Ensure the number of keys does not exceed the available keys\n",
    "    if num_keys > len(all_keys):\n",
    "        raise ValueError(\"num_keys exceeds the number of available keys in the dictionary.\")\n",
    "    \n",
    "    return random.sample(all_keys, num_keys)\n",
    "\n",
    " # Example dictionary with 500 keys\n",
    "output_file_name=k_10_semantic_NEP\n",
    "random_keys = select_random_keys(output_file_name, num_keys=100, seed=123)\n",
    "\n",
    "print(random_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_predictions(sentence, ground_truths, predictions):\n",
    "    updated_predictions = {}\n",
    "    all_prompts={}\n",
    "    # Iterate over each entity type in the predictions\n",
    "    for entity_type, prediction in predictions.items():\n",
    "        ground_truth = ground_truths.get(entity_type, [])\n",
    "        updated_predictions[entity_type] = [\"O\"] * len(prediction)\n",
    "        tokens = sentence.split()\n",
    "\n",
    "        i = 0\n",
    "        while i < len(prediction):\n",
    "            if prediction[i].startswith(\"B-\"):\n",
    "                # Start of a new entity span\n",
    "                entity_span = [tokens[i]]\n",
    "                entity_indices = [i]\n",
    "                entity_label = prediction[i][2:]  # Extract the label type\n",
    "\n",
    "                # Collect continuation tokens\n",
    "                j = i + 1\n",
    "                while j < len(prediction) and prediction[j] == f\"I-{entity_label}\":\n",
    "                    entity_span.append(tokens[j])\n",
    "                    entity_indices.append(j)\n",
    "                    j += 1\n",
    "\n",
    "                # Validate the entire entity span\n",
    "                entity_value = \" \".join(entity_span)\n",
    "\n",
    "                # Prompt GPT to validate\n",
    "                # prompt = f\"Is '{entity_value}' a {entity_type} in this sentence: '{sentence}'? Please answer with Yes or No. No explanation is needed.\"\n",
    "                if len(entity_span)>1:\n",
    "                    prompt=f\"The given sentence: {sentence} \\nAre the words {entity_value} in the given sentence a {entity_type} entity? Please answer with Yes or No. No explanation is needed. \"\n",
    "                else:\n",
    "                    prompt=f\"The given sentence: {sentence} \\nIs the word {entity_value} in the given sentence a {entity_type} entity? Please answer with Yes or No. No explanation is needed. \"\n",
    "                # print(prompt)\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    answer=generate_NER(\"openai\",prompt)\n",
    "                    # print(answer)\n",
    "                    all_prompts[prompt]=answer\n",
    "                    # Update the prediction based on GPT's response\n",
    "                    if \"Yes\" in answer:\n",
    "                        # print(\"I came here\")\n",
    "                        for idx in entity_indices:\n",
    "                            updated_predictions[entity_type][idx] = prediction[idx]\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error querying OpenAI: {e}\")\n",
    "                    # updated_predictions[entity_type].append(\"O\")\n",
    "            # else:\n",
    "            #     updated_predictions[entity_type].append(\"O\")\n",
    "            i+=1\n",
    "    return updated_predictions,all_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true1=[]\n",
    "y_pred1=[]\n",
    "y_pred2=[]\n",
    "count=0\n",
    "error_sentences={}\n",
    "count=0\n",
    "self_corrections={}\n",
    "original_predictions={}\n",
    "new_predictions={}\n",
    "for sentence in random_keys[:]:\n",
    "    true_label=ground_truth_separated[sentence]\n",
    "    temp={}\n",
    "    temp1={}\n",
    "    temp2={}\n",
    "    for entity in entities_list:\n",
    "\n",
    "        prediction=(output_file_name[sentence][entity])\n",
    "        prediction=post_process_output(prediction)\n",
    "\n",
    "        predicted_sentences=convert_to_bio(entity,prediction,sentence)\n",
    "       \n",
    "        if len(predicted_sentences)==len(true_label[entity]):\n",
    "\n",
    "            y_true1.append(true_label[entity])\n",
    "            y_pred1.append(predicted_sentences)\n",
    "            \n",
    "            validated_predictions,all_prompts = validate_predictions(sentence, {entity:true_label[entity]}, {entity:predicted_sentences})\n",
    "            y_pred2.append(validated_predictions[entity])\n",
    "            temp1[entity]=predicted_sentences\n",
    "            temp2[entity]=validated_predictions\n",
    "            temp[entity]=all_prompts\n",
    "    self_corrections[sentence]=temp\n",
    "    original_predictions[sentence]=temp1\n",
    "    new_predictions[sentence]=temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true1,y_pred1, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true1,y_pred2, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
